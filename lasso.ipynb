{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5b3104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep-by-step Lasso on a 3×2 toy set\\n-----------------------------------\\n Data (already centred enough for a demo):\\n\\n sample   x1   x2     y\\n   1      1    2      5\\n   2      0    1      2\\n   3     -1   -1     -2\\n\\n We use λ = 1.0 so you can clearly see the shrinkage.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step-by-step Lasso on a 3×2 toy set\n",
    "-----------------------------------\n",
    " Data (already centred enough for a demo):\n",
    "\n",
    " sample   x1   x2     y\n",
    "   1      1    2      5\n",
    "   2      0    1      2\n",
    "   3     -1   -1     -2\n",
    "\n",
    " We use λ = 1.0 so you can clearly see the shrinkage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adb651",
   "metadata": {},
   "source": [
    "Least Absolute Shrinkage and Selection Operator\n",
    "When Should You Use Lasso?\n",
    "When you have many features and only some are truly important.\n",
    "\n",
    "When you want to reduce overfitting and simplify your model.\n",
    "\n",
    "When you want to automatically select features without manual work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5684ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcea1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([[ 1,  2],\n",
    "              [ 0,  1],\n",
    "              [-1, -1]], dtype=float)\n",
    "y = np.array([ 5,  2, -2], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79443e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "n,p=x.shape\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055c6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=1\n",
    "w=np.zeros(p)\n",
    "b=0.0\n",
    "col_len = (x**2).sum(axis=0) / n     # ⟨x_j²⟩ for scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e99481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state  →  w = [0, 0],  b = 0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def S(x, l):                         # soft-threshold helper if x>lambda then sub , 0 if less , \n",
    "    return np.sign(x) * max(abs(x) - l, 0.0)\n",
    "print(\"Initial state  →  w = [0, 0],  b = 0\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162363ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] After bias update: b =  1.67\n"
     ]
    }
   ],
   "source": [
    "resid= y - (x @ w + b)\n",
    "b+= resid.mean()\n",
    "print(f\"[1] After bias update: b = {b:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] After w1 update:  corr= 2.33,  w1 =  2.00\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "r_j= y- (x @ w -x[:,j]* w[j]+b) #-> muting current contribution.\n",
    "corr=(x[:,j] @ r_j)/n\n",
    "w[j]=S(corr,lam) /col_len[j]\n",
    "print(f\"[1] After w1 update:  corr={corr:5.2f},  w1 = {w[j]:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb261b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] After w2 update:  corr= 1.56,  w2 =  0.28\n",
      "[1] End of sweep #1 → w = [2.   0.28],  b =  1.67\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "r_j = y - (x @ w - x[:, j]*w[j] + b)\n",
    "corr = (x[:, j] @ r_j) / n\n",
    "w[j] = S(corr, lam) / col_len[j]\n",
    "print(f\"[1] After w2 update:  corr={corr:5.2f},  w2 = {w[j]:5.2f}\")\n",
    "\n",
    "print(f\"[1] End of sweep #1 → w = {np.round(w,2)},  b = {b:5.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90485730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] After bias update: b =  1.48\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2)  SECOND SWEEP  (same steps again to show the drift)\n",
    "# ------------------------------------------------------------------\n",
    "# 2-a. bias\n",
    "resid = y - (x @ w + b)\n",
    "b += resid.mean()\n",
    "print(f\"[2] After bias update: b = {b:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e9ca99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] After w1 update:  corr= 2.06,  w1 =  1.58\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "r_j = y - (x @ w - x[:, j]*w[j] + b)\n",
    "corr = (x[:, j] @ r_j) / n\n",
    "w[j] = S(corr, lam) / col_len[j]\n",
    "print(f\"[2] After w1 update:  corr={corr:5.2f},  w1 = {w[j]:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b26511de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] After w2 update:  corr= 2.10,  w2 =  0.55\n",
      "[2] End of sweep #2 → w = [1.58 0.55],  b =  1.48\n",
      "==================================================\n",
      "Predictions: [ 4.16  2.03 -0.65]\n",
      "Residual mean (should be ~0): -0.1800411522633747\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "r_j = y - (x @ w - x[:, j]*w[j] + b)\n",
    "corr = (x[:, j] @ r_j) / n\n",
    "w[j] = S(corr, lam) / col_len[j]\n",
    "print(f\"[2] After w2 update:  corr={corr:5.2f},  w2 = {w[j]:5.2f}\")\n",
    "\n",
    "print(f\"[2] End of sweep #2 → w = {np.round(w,2)},  b = {b:5.2f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3)  Final quick check\n",
    "# ------------------------------------------------------------------\n",
    "y_hat = x @ w + b\n",
    "print(\"Predictions:\", np.round(y_hat, 2))\n",
    "print(\"Residual mean (should be ~0):\", (y - y_hat).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = np.array([[ 1,  2],\n",
    "              [ 0,  1],\n",
    "              [-1, -1]], dtype=float)\n",
    "\n",
    "w = np.array([1.99, 0.23])   # after our first sweep\n",
    "j = 0                        # look at feature x₁\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "X[:, j]        # → array([ 1.,  0., -1.])\n",
    "w[j]           # → 1.99  (scalar)\n",
    "X[:, j] * w[j] # → array([ 1.99,  0.  , -1.99])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X @ w               # full prediction from all features\n",
    "- X[:, j] * w[j]    # remove feature j's share\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In coordinate-descent Lasso we update one weight at a time.\n",
    "To know how much this feature should change, we first ‘mute’ its current contribution, look at the leftover error (the partial residual), see how well the feature lines up with that error, then apply soft-thresholding. That single step gives the closed-form optimum for that coordinate.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analogy you can use\n",
    "Imagine the model is a choir: each feature is a singer, its weight is their volume.\n",
    "\n",
    "Mute one singer → see what notes are missing (partial residual).\n",
    "\n",
    "How well could that singer fill the gap? → cor  relation.\n",
    "\n",
    "Charge an entry fee λ → if they can’t sing loud enough to justify the fee, keep them silent (weight = 0).\n",
    "\n",
    "Repeat for every singer until the song matches the target audience’s ears (low error) with the smallest possible choir (sparse model).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ae0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge: When you want to keep all features (just make them smaller).\n",
    "\n",
    "# Lasso: When you want some features to drop out entirely (become zero)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
